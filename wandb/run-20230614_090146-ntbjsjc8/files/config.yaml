wandb_version: 1

_wandb:
  desc: null
  value:
    code_path: code/src/train_model.py
    python_version: 3.9.16
    cli_version: 0.15.4
    framework: fastai
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1686733306.897735
    t:
      1:
      - 1
      - 4
      - 5
      - 53
      - 55
      - 89
      2:
      - 1
      - 4
      - 5
      - 53
      - 55
      - 89
      3:
      - 23
      4: 3.9.16
      5: 0.15.4
      8:
      - 5
Learner:
  desc: null
  value:
    loss_func:
      axis: -1
      flatten: true
      floatify: true
      is_2d: false
      _name: FlattenedLoss of MSELoss()
    opt_func: fastai.optimizer.Adam
    lr: 0.001
    splitter: fastai.torch_core.trainable_params
    metrics: null
    path: .
    model_dir: models
    wd: null
    wd_bn_bias: false
    train_bn: true
    moms:
    - 0.95
    - 0.85
    - 0.95
    default_cbs: true
    _name: <fastai.learner.Learner object at 0x7fcc4113c1f0>
TrainEvalCallback:
  desc: null
  value: true
Recorder:
  desc: null
  value:
    add_time: true
    train_metrics: false
    valid_metrics: true
CastToTensor:
  desc: null
  value: true
ProgressCallback:
  desc: null
  value: true
ShortEpochCallback:
  desc: null
  value: true
WandbCallback:
  desc: null
  value:
    log: null
    log_preds: false
    log_preds_every_epoch: false
    log_model: true
    model_name: null
    log_dataset: false
    dataset_name: null
    valid_dl: null
    n_preds: 36
    seed: 12345
    reorder: true
SaveModelCallback:
  desc: null
  value:
    fname: model
    every_epoch: false
    at_end: false
    with_opt: false
ParamScheduler:
  desc: null
  value: true
n_inp:
  desc: null
  value: 1
input 1 dim 1:
  desc: null
  value: 8
input 1 dim 2:
  desc: null
  value: 2
batch size:
  desc: null
  value: 8
batch per epoch:
  desc: null
  value: 10000
model parameters:
  desc: null
  value: 132396
device:
  desc: null
  value: cpu
frozen:
  desc: null
  value: false
frozen idx:
  desc: null
  value: 0
